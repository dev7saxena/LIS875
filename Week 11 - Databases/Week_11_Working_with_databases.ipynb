{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to a MySQL database server through SSH tunnel"
      ],
      "metadata": {
        "id": "g8SB2vhYasGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql\n",
        "!pip install paramiko\n",
        "!pip install sshtunnel"
      ],
      "metadata": {
        "id": "ew__8XVrZh4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGCUYwWEY8oQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "ac104340-35ed-4db7-e811-3df74caf9f2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 VERSION()\n",
              "0  8.0.31-0ubuntu0.20.04.1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6ed52db-2c28-4393-929d-afbfc027990d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VERSION()</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.0.31-0ubuntu0.20.04.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6ed52db-2c28-4393-929d-afbfc027990d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6ed52db-2c28-4393-929d-afbfc027990d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6ed52db-2c28-4393-929d-afbfc027990d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pymysql\n",
        "import paramiko\n",
        "import pandas as pd\n",
        "\n",
        "from paramiko import SSHClient\n",
        "from sshtunnel import SSHTunnelForwarder\n",
        "from os.path import expanduser\n",
        "\n",
        "sql_hostname = 'localhost'\n",
        "sql_username = 'lisstudent'\n",
        "sql_password = 'LIS875@student'\n",
        "sql_main_database = 'acm_dl'\n",
        "sql_port = 3306\n",
        "\n",
        "ssh_host = 'camelot.cs.wisc.edu'\n",
        "ssh_user = 'lisstudent'\n",
        "ssh_password = 'LIS875@student'\n",
        "ssh_port = 22\n",
        "\n",
        "with SSHTunnelForwarder(\n",
        "        (ssh_host, ssh_port),\n",
        "        ssh_username=ssh_user,\n",
        "        ssh_password=ssh_password,\n",
        "        remote_bind_address=(sql_hostname, sql_port)) as tunnel:\n",
        "\n",
        "    conn = pymysql.connect(host='127.0.0.1', user=sql_username,\n",
        "            passwd=sql_password, db=sql_main_database, charset='utf8mb4',\n",
        "            port=tunnel.local_bind_port)\n",
        "    query = '''SELECT VERSION();'''\n",
        "    data = pd.read_sql_query(query, conn)\n",
        "    conn.close()\n",
        "\n",
        "# tunnel will be closed automatically\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading database records using pandas"
      ],
      "metadata": {
        "id": "5xD_5cPYa_Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "import paramiko\n",
        "import pandas as pd\n",
        "\n",
        "from paramiko import SSHClient\n",
        "from sshtunnel import SSHTunnelForwarder\n",
        "from os.path import expanduser\n",
        "\n",
        "sql_hostname = 'localhost'\n",
        "sql_username = 'lisstudent'\n",
        "sql_password = 'LIS875@student'\n",
        "sql_main_database = 'acm_dl'\n",
        "sql_port = 3306\n",
        "\n",
        "ssh_host = 'camelot.cs.wisc.edu'\n",
        "ssh_user = 'lisstudent'\n",
        "ssh_password = 'LIS875@student'\n",
        "ssh_port = 22\n",
        "\n",
        "with SSHTunnelForwarder(\n",
        "        (ssh_host, ssh_port),\n",
        "        ssh_username=ssh_user,\n",
        "        ssh_password=ssh_password,\n",
        "        remote_bind_address=(sql_hostname, sql_port)) as tunnel:\n",
        "    conn = pymysql.connect(host='127.0.0.1', user=sql_username,\n",
        "            passwd=sql_password, db=sql_main_database, charset='utf8mb4',\n",
        "            port=tunnel.local_bind_port)\n",
        "    query = '''SELECT * FROM acm_article ;'''\n",
        "    data = pd.read_sql_query(query, conn)\n",
        "    conn.close()\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "rxe9qlMzbCeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "4s_63HLyT4fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading database records without pandas (default cursor)\n",
        "\n",
        "The default cursor will return results as a tuple of tuples (each record is a tuple, and the whole set of records is also stored as a tuple)."
      ],
      "metadata": {
        "id": "cJG6PB8uJUwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "import paramiko\n",
        "import pandas as pd\n",
        "\n",
        "from paramiko import SSHClient\n",
        "from sshtunnel import SSHTunnelForwarder\n",
        "from os.path import expanduser\n",
        "\n",
        "sql_hostname = 'localhost'\n",
        "sql_username = 'lisstudent'\n",
        "sql_password = 'LIS875@student'\n",
        "sql_main_database = 'acm_dl'\n",
        "sql_port = 3306\n",
        "\n",
        "ssh_host = 'camelot.cs.wisc.edu'\n",
        "ssh_user = 'lisstudent'\n",
        "ssh_password = 'LIS875@student'\n",
        "ssh_port = 22\n",
        "\n",
        "with SSHTunnelForwarder(\n",
        "        (ssh_host, ssh_port),\n",
        "        ssh_username=ssh_user,\n",
        "        ssh_password=ssh_password,\n",
        "        remote_bind_address=(sql_hostname, sql_port)) as tunnel:\n",
        "\n",
        "    conn = pymysql.connect(host='127.0.0.1', user=sql_username,\n",
        "            passwd=sql_password, db=sql_main_database, charset='utf8mb4',\n",
        "            port=tunnel.local_bind_port)\n",
        "    \n",
        "    query = '''SELECT * FROM acm_article LIMIT 100;'''\n",
        "    cursor = conn.cursor()\n",
        "    count = cursor.execute(query)\n",
        "    print(count)\n",
        "\n",
        "    data = cursor.fetchall()\n",
        "    \n",
        "    conn.close()\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "Ojpo08UmbCbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data)"
      ],
      "metadata": {
        "id": "ptmZXXc8bCYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7900aee-3791-48b3-a329-15a64bba802a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "nmU-zVHChp40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408c7a57-16b0-4f97-cf6c-59f9ef14840f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "id": "P9OYnOLmbCVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f7bd7b-a49f-4cc3-8115-bd584b46d96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1526752',\n",
              " '1526709',\n",
              " 2009,\n",
              " '04/20/2009',\n",
              " 'Less talk, more rock: automated organization of community-contributed collections of concert videos',\n",
              " 'We describe a system for synchronization and organization of user-contributed content from live music events. We start with a set of short video clips taken at a single event by multiple contributors, who were using a varied set of capture devices. Using audio fingerprints, we synchronize these clips such that overlapping clips can be displayed simultaneously. Furthermore, we use the timing and link structure generated by the synchronization algorithm to improve the findability and representation of the event content, including identifying key moments of interest and descriptive text for important captured segments of the show. We also identify the preferred audio track when multiple clips overlap. We thus create a much improved representation of the event that builds on the automatic content match. Our work demonstrates important principles in the use of content analysis techniques for social media content on the Web, and applies those principles in the domain of live music capture.',\n",
              " '81335492711;81100585927',\n",
              " 'Lyndon Kennedy;Mor Naaman',\n",
              " '1028549;-1',\n",
              " 'Yahoo! Research, Santa Clara, CA, USA;Rutgers University, New Brunswick, NY, USA')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using DictCursor\n",
        "\n",
        "Results will be stored as a list of dict (along with column names)."
      ],
      "metadata": {
        "id": "TFTcuyJ1K20j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "import paramiko\n",
        "import pandas as pd\n",
        "\n",
        "from paramiko import SSHClient\n",
        "from sshtunnel import SSHTunnelForwarder\n",
        "from os.path import expanduser\n",
        "\n",
        "sql_hostname = 'localhost'\n",
        "sql_username = 'lisstudent'\n",
        "sql_password = 'LIS875@student'\n",
        "sql_main_database = 'acm_dl'\n",
        "sql_port = 3306\n",
        "\n",
        "ssh_host = 'camelot.cs.wisc.edu'\n",
        "ssh_user = 'lisstudent'\n",
        "ssh_password = 'LIS875@student'\n",
        "ssh_port = 22\n",
        "\n",
        "with SSHTunnelForwarder(\n",
        "        (ssh_host, ssh_port),\n",
        "        ssh_username=ssh_user,\n",
        "        ssh_password=ssh_password,\n",
        "        remote_bind_address=(sql_hostname, sql_port)) as tunnel:\n",
        "\n",
        "    conn = pymysql.connect(host='127.0.0.1', user=sql_username,\n",
        "            passwd=sql_password, db=sql_main_database, charset='utf8mb4',\n",
        "            port=tunnel.local_bind_port)\n",
        "    \n",
        "    query = '''SELECT * FROM acm_article LIMIT 100;'''\n",
        "    cursor = conn.cursor(cursor=pymysql.cursors.DictCursor)\n",
        "    count = cursor.execute(query)\n",
        "    print(count)\n",
        "\n",
        "    data = cursor.fetchall()\n",
        "    \n",
        "    conn.close()\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "cuJUD-TlK2Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data)"
      ],
      "metadata": {
        "id": "oMR9WF9RbCS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cabeee9-eaf1-4658-c983-71203d849daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "id": "YpF2SHKEaZOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Unbuffered Cursors (e.g., SSCursor and SSDictCursor)\n",
        "\n",
        "Unbuffered cursor will read data as a stream. Instead of using fetchall() to load all the data into the memory, we may use fetchone() iteratively to scan the records from the first to the last (without storing any records in the memory).\n"
      ],
      "metadata": {
        "id": "xsSWuxzBLTzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "import paramiko\n",
        "import pandas as pd\n",
        "\n",
        "from paramiko import SSHClient\n",
        "from sshtunnel import SSHTunnelForwarder\n",
        "from os.path import expanduser\n",
        "\n",
        "sql_hostname = 'localhost'\n",
        "sql_username = 'lisstudent'\n",
        "sql_password = 'LIS875@student'\n",
        "sql_main_database = 'acm_dl'\n",
        "sql_port = 3306\n",
        "\n",
        "ssh_host = 'camelot.cs.wisc.edu'\n",
        "ssh_user = 'lisstudent'\n",
        "ssh_password = 'LIS875@student'\n",
        "ssh_port = 22\n",
        "\n",
        "with SSHTunnelForwarder(\n",
        "        (ssh_host, ssh_port),\n",
        "        ssh_username=ssh_user,\n",
        "        ssh_password=ssh_password,\n",
        "        remote_bind_address=(sql_hostname, sql_port)) as tunnel:\n",
        "\n",
        "    conn = pymysql.connect(host='127.0.0.1', user=sql_username,\n",
        "            passwd=sql_password, db=sql_main_database, charset='utf8mb4',\n",
        "            port=tunnel.local_bind_port)\n",
        "    \n",
        "    query = '''SELECT * FROM acm_article;'''\n",
        "    cursor = conn.cursor(cursor=pymysql.cursors.SSDictCursor)\n",
        "    count = cursor.execute(query)\n",
        "    \n",
        "    count = 0\n",
        "    rec = cursor.fetchone()\n",
        "    while rec!=None:\n",
        "      count += 1\n",
        "      if count%1000==0:\n",
        "        print('processed %d records'%count)\n",
        "      # process the record\n",
        "      rec = cursor.fetchone()\n",
        "      \n",
        "    conn.close()\n",
        "\n",
        "count"
      ],
      "metadata": {
        "id": "vUNTsBV7LS9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-class Exercise (10 min)\n",
        "\n",
        "Scan through all the database records in the acm_article table iteratively using an unbuffered cursor. Check if an article's list of authors includes James Allan. If yes, add the article's record into a list. Print out the list after finishing processing all the database records."
      ],
      "metadata": {
        "id": "eE4yBdlOOVfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "import paramiko\n",
        "import pandas as pd\n",
        "\n",
        "from paramiko import SSHClient\n",
        "from sshtunnel import SSHTunnelForwarder\n",
        "from os.path import expanduser\n",
        "\n",
        "sql_hostname = 'localhost'\n",
        "sql_username = 'lisstudent'\n",
        "sql_password = 'LIS875@student'\n",
        "sql_main_database = 'acm_dl'\n",
        "sql_port = 3306\n",
        "\n",
        "ssh_host = 'camelot.cs.wisc.edu'\n",
        "ssh_user = 'lisstudent'\n",
        "ssh_password = 'LIS875@student'\n",
        "ssh_port = 22\n",
        "\n",
        "with SSHTunnelForwarder(\n",
        "        (ssh_host, ssh_port),\n",
        "        ssh_username=ssh_user,\n",
        "        ssh_password=ssh_password,\n",
        "        remote_bind_address=(sql_hostname, sql_port)) as tunnel:\n",
        "\n",
        "    conn = pymysql.connect(host='127.0.0.1', user=sql_username,\n",
        "            passwd=sql_password, db=sql_main_database, charset='utf8mb4',\n",
        "            port=tunnel.local_bind_port)\n",
        "    \n",
        "    query = '''SELECT * FROM acm_article;'''\n",
        "    cursor = conn.cursor(cursor=pymysql.cursors.SSDictCursor)\n",
        "    count = cursor.execute(query)\n",
        "    \n",
        "    data = []\n",
        "    count = 0\n",
        "    rec = cursor.fetchone()\n",
        "    while rec!=None:\n",
        "      \n",
        "      # process the record\n",
        "      if 'James Allan' in rec['art_au_text']:\n",
        "        data.append(rec)\n",
        "\n",
        "      count += 1\n",
        "      if count%10000==0:\n",
        "        print('processed %d records'%count)\n",
        "      # process the record\n",
        "\n",
        "      rec = cursor.fetchone()\n",
        "      \n",
        "    conn.close()\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "tVtIuT5CL2pR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060e9d90-c568-44ab-87e2-2cea94f23076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 10000 records\n",
            "processed 20000 records\n",
            "processed 30000 records\n",
            "processed 40000 records\n",
            "processed 50000 records\n",
            "processed 60000 records\n",
            "processed 70000 records\n",
            "processed 80000 records\n",
            "processed 90000 records\n",
            "processed 100000 records\n",
            "processed 110000 records\n",
            "processed 120000 records\n",
            "processed 130000 records\n",
            "processed 140000 records\n",
            "processed 150000 records\n",
            "processed 160000 records\n",
            "processed 170000 records\n",
            "processed 180000 records\n",
            "processed 190000 records\n",
            "processed 200000 records\n",
            "processed 210000 records\n",
            "processed 220000 records\n",
            "processed 230000 records\n",
            "processed 240000 records\n",
            "processed 250000 records\n",
            "processed 260000 records\n",
            "processed 270000 records\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'article_id': '1572050',\n",
              "  'proc_id': '1571941',\n",
              "  'art_pub_year': 2009,\n",
              "  'art_pub_date': '07/19/2009',\n",
              "  'art_title': 'Agreement among statistical significance tests for information retrieval evaluation at varying sample sizes',\n",
              "  'art_abstract': \"Research has shown that little practical difference exists between the randomization, Student's paired t, and bootstrap tests of statistical significance for TREC ad-hoc retrieval experiments with 50 topics. We compared these three tests on runs with topic sizes down to 10 topics. We found that these tests show increasing disagreement as the number of topics decreases. At smaller numbers of topics, the randomization test tended to produce smaller p-values than the t-test for p-values less than 0.1. The bootstrap exhibited a systematic bias towards p-values strictly less than the t-test with this bias increasing as the number of topics decreased. We recommend the use of the randomization test although the t-test appears to be suitable even when the number of topics is small.\",\n",
              "  'art_au_id': '81100457054;81342487699;81100270582',\n",
              "  'art_au_text': 'Mark D. Smucker;James Allan;Ben Carterette',\n",
              "  'art_inst_id': '1026693;1026103;1025858',\n",
              "  'art_inst_text': 'University of Waterloo, Waterloo, ON, Canada;University of Massachusetts Amherst, Amherst, MA, USA;University of Delaware, Newark, DE, USA'},\n",
              " {'article_id': '1835458',\n",
              "  'proc_id': '1835449',\n",
              "  'art_pub_year': 2010,\n",
              "  'art_pub_date': '07/19/2010',\n",
              "  'art_title': 'Predicting searcher frustration',\n",
              "  'art_abstract': 'When search engine users have trouble finding information, they may become frustrated, possibly resulting in a bad experience (even if they are ultimately successful). In a user study in which participants were given difficult information seeking tasks, half of all queries submitted resulted in some degree of self-reported frustration. A third of all successful tasks involved at least one instance of frustration. By modeling searcher frustration, search engines can predict the current state of user frustration and decide when to intervene with alternative search strategies to prevent the user from becoming more frustrated, giving up, or switching to another search engine. We present several models to predict frustration using features extracted from query logs and physical sensors. We are able to predict frustration with a mean average precision of 65\\\\% from the physical sensors, and 87\\\\% from the query log features.',\n",
              "  'art_au_id': '81323490009;81342487699;81335492485',\n",
              "  'art_au_text': 'Henry A. Feild;James Allan;Rosie Jones',\n",
              "  'art_inst_id': '1026103;1026103;-1',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;Yahoo! Labs, Cambridge, MA, USA'},\n",
              " {'article_id': '1835521',\n",
              "  'proc_id': '1835449',\n",
              "  'art_pub_year': 2010,\n",
              "  'art_pub_date': '07/19/2010',\n",
              "  'art_title': 'A content based approach for discovering missing anchor text for web search',\n",
              "  'art_abstract': \"Although anchor text provides very useful information for web search, a large portion of web pages have few or no incoming hyperlinks (anchors), which is known as the anchor text sparsity problem. In this paper, we propose a language modeling based technique for overcoming anchor text sparsity by discovering a web page's plausible missing anchor text from its similar web pages' in-link anchor text. We design experiments with two publicly available TREC web corpora (GOV2 and ClueWeb09) to evaluate different approaches for discovering missing anchor text. Experimental results show that our approach can effectively discover plausible missing anchor terms. We then use the web named page finding task in the TREC Terabyte track to explore the utility of missing anchor text information discovered by our approach for helping retrieval. Experimental results show that our approach can statistically significantly improve retrieval performance, compared with several approaches that only use anchor text aggregated over the web graph.\",\n",
              "  'art_au_id': '81100168709;81342487699',\n",
              "  'art_au_text': 'Xing Yi;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Computer Science Department, University of Massachusetts, Amherst, Amherst, MA, USA;Center for Intelligent Information Retrieval, Computer Science Department, University of Massachusetts, Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '1835650',\n",
              "  'proc_id': '1835449',\n",
              "  'art_pub_year': 2010,\n",
              "  'art_pub_date': '07/19/2010',\n",
              "  'art_title': 'Learning to select rankers',\n",
              "  'art_abstract': 'Combining evidence from multiple retrieval models has been widely studied in the context of of distributed search, metasearch and rank fusion. Much of the prior work has focused on combining retrieval scores (or the rankings) assigned by different retrieval models or ranking algorithms. In this work, we focus on the problem of choosing between retrieval models using performance estimation. We propose modeling the differences in retrieval performance directly by using rank-time features - features that are available to the ranking algorithms - and the retrieval scores assigned by the ranking algorithms. Our experimental results show that when choosing between two rankers, our approach yields significant improvements over the best individual ranker.',\n",
              "  'art_au_id': '81335487752;81342487699',\n",
              "  'art_au_text': 'Niranjan Balasubramanian;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '188586',\n",
              "  'proc_id': '188490',\n",
              "  'art_pub_year': 1994,\n",
              "  'art_pub_date': '08/01/1994',\n",
              "  'art_title': 'The effect of adding relevance information in a relevance feedback environment',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81100350752;81100649307;81342487699',\n",
              "  'art_au_text': 'Chris Buckley;Gerard Salton;James Allan',\n",
              "  'art_inst_id': '1005775;1005775;1005775',\n",
              "  'art_inst_text': 'Cornell Univ., Ithaca, NY;Cornell Univ., Ithaca, NY;Cornell Univ., Ithaca, NY'},\n",
              " {'article_id': '2009969',\n",
              "  'proc_id': '2009916',\n",
              "  'art_pub_year': 2011,\n",
              "  'art_pub_date': '07/24/2011',\n",
              "  'art_title': 'CrowdLogging: distributed, private, and anonymous search logging',\n",
              "  'art_abstract': \"We describe CrowdLogging, an approach for distributed search log collection, storage, and mining, with the dual goals of preserving privacy and making the mined information broadly available. Most search log mining approaches and most privacy enhancing schemes have focused on centralized search logs and methods for disseminating them to third parties. In our approach, a user's search log is encrypted and shared in such a way that (a) the source of a search behavior artifact, such as a query, is unknown and (b) extremely rare artifacts---that is, artifacts more likely to contain private information---are not revealed. The approach works with any search behavior artifact that can be extracted from a search log, including queries, query reformulations, and query-click pairs. In this work, we: (1) present a distributed search log collection, storage, and mining framework; (2) compare several privacy policies, including differential privacy, showing the trade-offs between strong guarantees and the utility of the released data; (3) demonstrate the impact of our approach using two existing research query logs; and (4) describe a pilot study for which we implemented a version of the framework.\",\n",
              "  'art_au_id': '81323490009;81342487699;81487655016',\n",
              "  'art_au_text': 'Henry Allen Feild;James Allan;Joshua Glatt',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '215380',\n",
              "  'proc_id': '215206',\n",
              "  'art_pub_year': 1995,\n",
              "  'art_pub_date': '07/01/1995',\n",
              "  'art_title': 'Relevance feedback with too much data',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81342487699',\n",
              "  'art_au_text': 'James Allan',\n",
              "  'art_inst_id': '1026103',\n",
              "  'art_inst_text': 'Department of Computer Science, University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '243274',\n",
              "  'proc_id': '243199',\n",
              "  'art_pub_year': 1996,\n",
              "  'art_pub_date': '08/18/1996',\n",
              "  'art_title': 'Incremental relevance feedback for information filtering',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81342487699',\n",
              "  'art_au_text': 'James Allan',\n",
              "  'art_inst_id': '1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '1009044',\n",
              "  'proc_id': '1008992',\n",
              "  'art_pub_year': 2004,\n",
              "  'art_pub_date': '07/25/2004',\n",
              "  'art_title': 'Text classification and named entities for new event detection',\n",
              "  'art_abstract': 'New Event Detection is a challenging task that still offers scope for great improvement after years of effort. In this paper we show how performance on New Event Detection (NED) can be improved by the use of text classification techniques as well as by using named entities in a new way. We explore modifications to the document representation in a vector space-based NED system. We also show that addressing named entities preferentially is useful only in certain situations. A combination of all the above results in a multi-stage NED system that performs much better than baseline single-stage NED systems.',\n",
              "  'art_au_id': '81340490736;81342487699',\n",
              "  'art_au_text': 'Giridhar Kumaran;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1076109',\n",
              "  'proc_id': '1076034',\n",
              "  'art_pub_year': 2005,\n",
              "  'art_pub_date': '08/15/2005',\n",
              "  'art_title': 'When will information retrieval be good enough?',\n",
              "  'art_abstract': 'We describe a user study that examined the relationship between the quality of an Information Retrieval system and the effectiveness of its users in performing a task. The task involves finding answer facets of questions pertaining to a collection of newswire documents over a six month period. We artificially created sets of ranked lists at increasing levels of quality by blending the output of a state-of-the-art retrieval system with truth data created by annotators. Subjects performed the task by using these ranked lists to guide their labeling of answer passages in the retrieved articles. We found that as system accuracy improves, subject time on task and error rate decrease, and the rate of finding new correct answers increases. There is a large intermediary region in which the utility difference is not significant; our results suggest that there is some threshold of accuracy for this task beyond which user utility improves rapidly, but more experiments are needed to examine the area around that threshold closely.',\n",
              "  'art_au_id': '81342487699;81100270582;81375614037',\n",
              "  'art_au_text': 'James Allan;Ben Carterette;Joshua Lewis',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1148219',\n",
              "  'proc_id': '1148170',\n",
              "  'art_pub_year': 2006,\n",
              "  'art_pub_date': '08/06/2006',\n",
              "  'art_title': 'Minimal test collections for retrieval evaluation',\n",
              "  'art_abstract': 'Accurate estimation of information retrieval evaluation metrics such as average precision require large sets of relevance judgments. Building sets large enough for evaluation of real-world implementations is at best inefficient, at worst infeasible. In this work we link evaluation with test collection construction to gain an understanding of the minimal judging effort that must be done to have high confidence in the outcome of an evaluation. A new way of looking at average precision leads to a natural algorithm for selecting documents to judge and allows us to estimate the degree of confidence by defining a distribution over possible document judgments. A study with annotators shows that this method can be used by a small group of researchers to rank a set of systems in under three hours with 95\\\\% confidence. Information retrieval metrics such as average precision require large sets of relevance judgments to be accurately estimated. Building these sets is infeasible and often inefficient for many real-world retrieval implementations. We present a new way of looking at average precision that allows us to estimate the confidence in an evaluation based on the size of the test collection. We use this to build an algorithm for selecting the best documents to judge to have maximum confidence in an evaluation with a minimal number of relevance judgments. A study with annotators shows how the algorithm can be used by a small group of researchers to quickly rank a set of systems with 95\\\\% confidence.',\n",
              "  'art_au_id': '81100270582;81342487699;81100324014',\n",
              "  'art_au_text': 'Ben Carterette;James Allan;Ramesh Sitaraman',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1148250',\n",
              "  'proc_id': '1148170',\n",
              "  'art_pub_year': 2006,\n",
              "  'art_pub_date': '08/06/2006',\n",
              "  'art_title': 'Find-similar: similarity browsing as a search tool',\n",
              "  'art_abstract': \"Search systems have for some time provided users with the ability to request documents similar to a given document. Interfaces provide this feature via a link or button for each document in the search results. We call this feature find-similar or similarity browsing . We examined find-similar as a search tool, like relevance feedback, for improving retrieval performance. Our investigation focused on find-similar's document-to-document similarity, the reexamination of documents during a search, and the user's browsing pattern. Find-similar with a query-biased similarity, avoiding the reexamination of documents, and a breadth-like browsing pattern achieved a 23\\\\% increase in the arithmetic mean average precision and a 66\\\\% increase in the geometric mean average precision over our baseline retrieval. This performance matched that of a more traditionally styled iterative relevance feedback technique.\",\n",
              "  'art_au_id': '81100457054;81342487699',\n",
              "  'art_au_text': 'Mark D. Smucker;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst;University of Massachusetts Amherst'},\n",
              " {'article_id': '1148305',\n",
              "  'proc_id': '1148170',\n",
              "  'art_pub_year': 2006,\n",
              "  'art_pub_date': '08/06/2006',\n",
              "  'art_title': 'Simple questions to improve pseudo-relevance feedback results',\n",
              "  'art_abstract': 'We explore interactive methods to further improve the performance of pseudo-relevance feedback. Studies \\\\citeria suggest that new methods for tackling difficult queries are required. Our approach is to gather more information about the query from the user by asking her simple questions. The equally simple responses are used to modify the original query. Our experiments using the TREC Robust Track queries show that we can obtain a significant improvement in mean average precision averaging around 5\\\\% over pseudo-relevance feedback. This improvement is also spread across more queries compared to ordinary pseudo-relevance feedback, as suggested by geometric mean average precision.',\n",
              "  'art_au_id': '81340490736;81342487699',\n",
              "  'art_au_text': 'Giridhar Kumaran;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1148324',\n",
              "  'proc_id': '1148170',\n",
              "  'art_pub_year': 2006,\n",
              "  'art_pub_date': '08/06/2006',\n",
              "  'art_title': 'Lightening the load of document smoothing for better language modeling retrieval',\n",
              "  'art_abstract': 'We hypothesized that language modeling retrieval would improve if we reduced the need for document smoothing to provide an inverse document frequency (IDF) like effect. We created inverse collection frequency (ICF) weighted query models as a tool to partially separate the IDF-like role from document smoothing. Compared to maximum likelihood estimated (MLE) queries, the ICF weighted queries achieved a 6.4\\\\\\\\% improvement in mean average precision on description queries. The ICF weighted queries performed better with less document smoothing than that required by MLE queries. Language modeling retrieval may benefit from a means to separately incorporate an IDF-like behavior outside of document smoothing.',\n",
              "  'art_au_id': '81100457054;81342487699',\n",
              "  'art_au_text': 'Mark D. Smucker;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '1277758',\n",
              "  'proc_id': '1277741',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '07/23/2007',\n",
              "  'art_title': 'An interactive algorithm for asking and incorporating feature feedback into support vector machines',\n",
              "  'art_abstract': 'Standard machine learning techniques typically require ample training data in the form of labeled instances. In many situations it may be too tedious or costly to obtain sufficient labeled data for adequate classifier performance. However, in text classification, humans can easily guess the relevance of features, that is, words that are indicative of a topic, thereby enabling the classifier to focus its feature weights more appropriately in the absence of sufficient labeled data. We will describe an algorithm for tandem learning that begins with a couple of labeled instances, and then at each iteration recommends features and instances for a human to label. Tandem learning using an \"oracle\" results in much better performance than learning on only features or only instances. We find that humans can emulate the oracle to an extent that results in performance (accuracy) comparable to that of the oracle. Our unique experimental design helps factor out system error from human error, leading to a better understanding of when and why interactive feature selection works.',\n",
              "  'art_au_id': '81339523170;81342487699',\n",
              "  'art_au_text': 'Hema Raghavan;James Allan',\n",
              "  'art_inst_id': '1028549;-1',\n",
              "  'art_inst_text': 'Yahoo! Inc;University of Massachusetts'},\n",
              " {'article_id': '1277920',\n",
              "  'proc_id': '1277741',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '07/23/2007',\n",
              "  'art_title': 'Matching resumes and jobs based on relevance models',\n",
              "  'art_abstract': 'We investigate the difficult problem of matching semi-structured resumes and jobs in a large scale real-world collection. We compare standard approaches to Structured Relevance Models (SRM), an extensionof relevance-based language model for modeling and retrieving semi-structured documents. Preliminary experiments show that the SRM approach achieved promising performance and performed better than typical unstructured relevance models.',\n",
              "  'art_au_id': '81100168709;81342487699;81100652508',\n",
              "  'art_au_text': 'Xing Yi;James Allan;W. Bruce Croft',\n",
              "  'art_inst_id': '-1;-1;-1',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Amherst, MA;Center for Intelligent Information Retrieval, Amherst, MA;Center for Intelligent Information Retrieval, Amherst, MA'},\n",
              " {'article_id': '1277922',\n",
              "  'proc_id': '1277741',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '07/23/2007',\n",
              "  'art_title': 'A comparison of sentence retrieval techniques',\n",
              "  'art_abstract': 'Identifying redundant information in sentences is useful for several applications such as summarization, document provenance, detecting text reuse and novelty detection. The task of identifying redundant information in sentences is defined as follows: Given a query sentence the task is to retrieve sentences from a given collection that express all or some subset of the information present in the query sentence. Sentence retrieval techniques rank sentences based on some measure of their similarity to a query. The effectiveness of such techniques depends on the similarity measure used to rank sentences. An effective retrieval model should be able to handle low word overlap between query and candidate sentences and go beyond just word overlap. Simple language modeling techniques like query likelihood retrieval have outperformed TF-IDF and word overlap based methods for ranking sentences. In this paper, we compare the performance of sentence retrieval using different language modeling techniques for the problem of identifying redundant information.',\n",
              "  'art_au_id': '81335487752;81342487699;81100652508',\n",
              "  'art_au_text': 'Niranjan Balasubramanian;James Allan;W. Bruce Croft',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1277947',\n",
              "  'proc_id': '1277741',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '07/23/2007',\n",
              "  'art_title': 'Using similarity links as shortcuts to relevant web pages',\n",
              "  'art_abstract': 'Successful navigation from a relevant web page to other relevant pages depends on the page linking to other relevant pages. We measured the distance to travel from relevant page to relevant page and found a bimodal distribution of distances peaking at 4 and 15 hops. In an attempt to make it easier to navigate among relevant pages, we added content similarity links to pages. With these additional links, significantly more relevant documents were close to each other. A browser plug-in or other tool that provides links to pages similar to a given page should increase the ability of web users to find relevant pages via navigation.',\n",
              "  'art_au_id': '81100457054;81342487699',\n",
              "  'art_au_text': 'Mark D. Smucker;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1390339',\n",
              "  'proc_id': '1390334',\n",
              "  'art_pub_year': 2008,\n",
              "  'art_pub_date': '07/20/2008',\n",
              "  'art_title': 'Effective and efficient user interaction for long queries',\n",
              "  'art_abstract': 'Handling long queries can involve either pruning the query to retain only the important terms (reduction), or expanding the query to include related concepts (expansion). While automatic techniques to do so exist, roughly 25\\\\% performance improvements in terms of MAP have been realized in past work through interactive variants. We show that selectively reducing or expanding a query leads to an average improvement of 51\\\\% in MAP over the baseline for standard TREC test collections. We demonstrate how user interaction can be used to achieve this improvement. Most interaction techniques present users with a fixed number of options for all queries. We achieve improvements by interacting less with the user, i.e., we present techniques to identify the optimal number of options to present to users, resulting in an interface with an average of 70\\\\% fewer options to consider. Previous algorithms supporting interactive reduction and expansion are exponential in nature. To extend their utility to operational environments, we present techniques to make the complexity of the algorithms polynomial. We finally present an analysis of long queries that continue to exhibit poor performance in spite of our new techniques.',\n",
              "  'art_au_id': '81340490736;81342487699',\n",
              "  'art_au_text': 'Giridhar Kumaran;James Allan',\n",
              "  'art_inst_id': '1015014;1026103',\n",
              "  'art_inst_text': 'Microsoft Live Labs, Redmond, WA, USA;University of Massachusetts, Amherst, MA, USA'},\n",
              " {'article_id': '1390376',\n",
              "  'proc_id': '1390334',\n",
              "  'art_pub_year': 2008,\n",
              "  'art_pub_date': '07/20/2008',\n",
              "  'art_title': 'A cluster-based resampling method for pseudo-relevance feedback',\n",
              "  'art_abstract': 'Typical pseudo-relevance feedback methods assume the top-retrieved documents are relevant and use these pseudo-relevant documents to expand terms. The initial retrieval set can, however, contain a great deal of noise. In this paper, we present a cluster-based resampling method to select better pseudo-relevant documents based on the relevance model. The main idea is to use document clusters to find dominant documents for the initial retrieval set, and to repeatedly feed the documents to emphasize the core topics of a query. Experimental results on large-scale web TREC collections show significant improvements over the relevance model. For justification of the resampling approach, we examine relevance density of feedback documents. A higher relevance density will result in greater retrieval accuracy, ultimately approaching true relevance feedback. The resampling approach shows higher relevance density than the baseline relevance model on all collections, resulting in better retrieval accuracy in pseudo-relevance feedback. This result indicates that the proposed method is effective for pseudo-relevance feedback.',\n",
              "  'art_au_id': '81406597621;81100652508;81342487699',\n",
              "  'art_au_text': 'Kyung Soon Lee;W. Bruce Croft;James Allan',\n",
              "  'art_inst_id': '1004769;1026103;1026103',\n",
              "  'art_inst_text': 'Chonbuk National University, Jeonju, South Korea;University of Massachusetts Amherst, Amherst, USA;University of Massachusetts Amherst, Amherst, USA'},\n",
              " {'article_id': '1390445',\n",
              "  'proc_id': '1390334',\n",
              "  'art_pub_year': 2008,\n",
              "  'art_pub_date': '07/20/2008',\n",
              "  'art_title': 'Evaluation over thousands of queries',\n",
              "  'art_abstract': 'Information retrieval evaluation has typically been performed over several dozen queries, each judged to near-completeness. There has been a great deal of recent work on evaluation over much smaller judgment sets: how to select the best set of documents to judge and how to estimate evaluation measures when few judgments are available. In light of this, it should be possible to evaluate over many more queries without much more total judging effort. The Million Query Track at TREC 2007 used two document selection algorithms to acquire relevance judgments for more than 1,800 queries. We present results of the track, along with deeper analysis: investigating tradeoffs between the number of queries and number of judgments shows that, up to a point, evaluation over more queries with fewer judgments is more cost-effective and as reliable as fewer queries with more judgments. Total assessor effort can be reduced by 95\\\\% with no appreciable increase in evaluation errors.',\n",
              "  'art_au_id': '81100270582;81418597119;81100071997;81100398599;81342487699',\n",
              "  'art_au_text': 'Ben Carterette;Virgil Pavlu;Evangelos Kanoulas;Javed A. Aslam;James Allan',\n",
              "  'art_inst_id': '1026103;1033067;1033067;1033067;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;Northeastern University, Boston, MA, USA;Northeastern University, Boston, MA, USA;Northeastern University, Boston, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '290954',\n",
              "  'proc_id': '290941',\n",
              "  'art_pub_year': 1998,\n",
              "  'art_pub_date': '08/01/1998',\n",
              "  'art_title': 'On-line new event detection and tracking',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81342487699;81100034951;81100151204',\n",
              "  'art_au_text': 'James Allan;Ron Papka;Victor Lavrenko',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Computer Science Department, University of Massachusetts, Amherst, MA;Center for Intelligent Information Retrieval, Computer Science Department, University of Massachusetts, Amherst, MA;Center for Intelligent Information Retrieval, Computer Science Department, University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '290987',\n",
              "  'proc_id': '290941',\n",
              "  'art_pub_year': 1998,\n",
              "  'art_pub_date': '08/01/1998',\n",
              "  'art_title': 'Aspect windows, 3-D visualizations, and indirect comparisons of information retrieval systems',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81375604095;81342487699',\n",
              "  'art_au_text': 'Russell C. Swan;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, MA;Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '291043',\n",
              "  'proc_id': '290941',\n",
              "  'art_pub_year': 1998,\n",
              "  'art_pub_date': '08/01/1998',\n",
              "  'art_title': 'Visual interactions with a multidimensional ranked list',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81377592914;81342487699',\n",
              "  'art_au_text': 'Anton Leouski;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, MA;Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '345674',\n",
              "  'proc_id': '345508',\n",
              "  'art_pub_year': 2000,\n",
              "  'art_pub_date': '07/01/2000',\n",
              "  'art_title': 'TimeMine (demonstration session): visualizing automatically constructed timelines',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81375604095;81342487699',\n",
              "  'art_au_text': 'Russell Swan;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, Massachusetts;Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, Massachusetts'},\n",
              " {'article_id': '345546',\n",
              "  'proc_id': '345508',\n",
              "  'art_pub_year': 2000,\n",
              "  'art_pub_date': '07/01/2000',\n",
              "  'art_title': 'Automatic generation of overview timelines',\n",
              "  'art_abstract': 'We present a statistical model of feature occurrence over time, and develop tests based on classical hypothesis testing for significance of term appearance on a given date. Using additional classical hypothesis testing we are able to combine these terms to generate “topics” as defined by the Topic Detection and Tracking study. The groupings of terms obtained can be used to automatically generate an interactive timeline displaying the major events and topics covered by the corpus. To test the validity of our technique we extracted a large number of these topics from a test corpus and had human evaluators judge how well the selected features captured the gist of the topics, and how they overlapped with a set of known topics from the corpus. The resulting topics were highly rated by evaluators who compared them to known topics.',\n",
              "  'art_au_id': '81375604095;81342487699',\n",
              "  'art_au_text': 'Russell Swan;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, Massachusetts;Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, Massachusetts'},\n",
              " {'article_id': '544264',\n",
              "  'proc_id': '544220',\n",
              "  'art_pub_year': 2002,\n",
              "  'art_pub_date': '07/14/2002',\n",
              "  'art_title': 'Core services in the architecture of the national science digital library (NSDL)',\n",
              "  'art_abstract': 'We describe the core components of the architecture for the National Science Digital Library (NSDL). Over time the NSDL will include heterogeneous users, content, and services. To accommodate this, a design for a technical and organization infrastructure has been formulated based on the notion of a spectrum of interoperability. This paper describes the first phase of the interoperability infrastructure including the metadata repository, search and discovery services, rights management services, and user interface portal facilities.',\n",
              "  'art_au_id': '81100645517;81100548454;81377592394;81100358216;81377591167;81100188380;81100155705;81100643525;81406594157;81100142068;81377591090;81100640218;81342487699;81375612075;81315489458',\n",
              "  'art_au_text': 'Carl Lagoze;William Arms;Stoney Gan;Diane Hillmann;Christopher Ingram;Dean Krafft;Richard Marisa;Jon Phipps;John Saylor;Carol Terrizzi;Walter Hoehn;David Millman;James Allan;Sergio Guzman-Lara;Tom Kalt',\n",
              "  'art_inst_id': '1005775;1005775;1005775;1005775;1005775;1005775;1005775;1005775;1005775;1005775;1005362;1005362;1026103;1026103;1026103',\n",
              "  'art_inst_text': 'Cornell University, Ithaca, NY;Cornell University, Ithaca, NY;Cornell University, Ithaca, NY;Cornell University, Ithaca, NY;Cornell University, Ithaca, NY;Cornell University, Ithaca, NY;Cornell University, Ithaca, NY;Cornell University, Ithaca, NY;Cornell University, Ithaca, NY;Cornell University, Ithaca, NY;Columbia University, NY, NY;Columbia University, NY, NY;University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '564394',\n",
              "  'proc_id': '564376',\n",
              "  'art_pub_year': 2002,\n",
              "  'art_pub_date': '08/11/2002',\n",
              "  'art_title': 'Improving realism of topic tracking evaluation',\n",
              "  'art_abstract': 'Topic tracking and information filtering are models of interactive tasks, but their evaluations are generally done in a way that does not reflect likely usage. The models either force frequent judgments or disallow any at all, assume the user is always available to make a judgment, and do not allow for user fatigue. In this study we extend the evaluation framework for topic tracking to incorporate those more realistic issues. We demonstrate that tracking can be done in a realistic interactive setting with minimal impact on tracking cost and with substantial reduction in required interaction.',\n",
              "  'art_au_id': '81100344761;81342487699',\n",
              "  'art_au_text': 'Anton Leuski;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '564430',\n",
              "  'proc_id': '564376',\n",
              "  'art_pub_year': 2002,\n",
              "  'art_pub_date': '08/11/2002',\n",
              "  'art_title': 'Using part-of-speech patterns to reduce query ambiguity',\n",
              "  'art_abstract': 'Query ambiguity is a generally recognized problem, particularly in Web environments where queries are commonly only one or two words in length. In this study, we explore one technique that finds commonly occurring patterns of parts of speech near a one-word query and allows them to be transformed into clarification questions. We use a technique derived from statistical language modeling to show that the clarification queries will reduce ambiguity much of the time, and often quite substantially.',\n",
              "  'art_au_id': '81342487699;81339523170',\n",
              "  'art_au_text': 'James Allan;Hema Raghavan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '564465',\n",
              "  'proc_id': '564376',\n",
              "  'art_pub_year': 2002,\n",
              "  'art_pub_date': '08/11/2002',\n",
              "  'art_title': \"A critical examination of TDT's cost function\",\n",
              "  'art_abstract': 'Topic Detection and Tracking (TDT) tasks are evaluated using a cost function. The standard TDT cost function assumes a constant probability of relevance P (rel) across all topics. In practice, P (rel) varies widely across topics. We argue using both theoretical and experimental evidence that the cost function should be modified to account for the varying P (rel).',\n",
              "  'art_au_id': '81332514056;81100140749;81342487699',\n",
              "  'art_au_text': 'R. Manmatha;Ao Feng;James Allan',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '383954',\n",
              "  'proc_id': '383952',\n",
              "  'art_pub_year': 2001,\n",
              "  'art_pub_date': '09/01/2001',\n",
              "  'art_title': 'Temporal summaries of new topics',\n",
              "  'art_abstract': 'We discuss technology to help a person monitor changes in news coverage over time. We define temporal summaries of news stories as extracting a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. We explain a method for evaluation, and describe an evaluation corpus that we have built. We also propose several methods for constructing temporal summaries and evaluate their effectiveness in comparison to degenerate cases. We show that simple approaches are effective, but that the problem is far from solved.',\n",
              "  'art_au_id': '81342487699;81337489552;81375610793',\n",
              "  'art_au_text': 'James Allan;Rahul Gupta;Vikas Khandelwal',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'Univ. of Massachusetts, Amherst;Univ. of Massachusetts, Amherst;Univ. of Massachusetts, Amherst'},\n",
              " {'article_id': '860493',\n",
              "  'proc_id': '860435',\n",
              "  'art_pub_year': 2003,\n",
              "  'art_pub_date': '07/28/2003',\n",
              "  'art_title': 'Retrieval and novelty detection at the sentence level',\n",
              "  'art_abstract': 'Previous research in novelty detection has focused on the task of finding novel material, given a set or stream of documents on a certain topic. This study investigates the more difficult two-part task defined by the TREC 2002 novelty track: given a topic and a group of documents relevant to that topic, 1) find the relevant sentences from the documents, and 2) find the novel sentences from the collection of relevant sentences. Our research shows that the former step appears to be the more difficult part of this task, and that the performance of novelty measures is very sensitive to the presence of non-relevant sentences.',\n",
              "  'art_au_id': '81342487699;81375612342;81100124635',\n",
              "  'art_au_text': 'James Allan;Courtney Wade;Alvaro Bolivar',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '860548',\n",
              "  'proc_id': '860435',\n",
              "  'art_pub_year': 2003,\n",
              "  'art_pub_date': '07/28/2003',\n",
              "  'art_title': 'Stemming in the language modeling framework',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81342487699;81340490736',\n",
              "  'art_au_text': 'James Allan;Giridhar Kumaran',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1646118',\n",
              "  'proc_id': '1645953',\n",
              "  'art_pub_year': 2009,\n",
              "  'art_pub_date': '11/02/2009',\n",
              "  'art_title': 'Incident threading for news passages',\n",
              "  'art_abstract': 'With an overwhelming volume of news reports currently available, there is an increasing need for automatic techniques to analyze and present news to a general reader in a meaningful and efficient manner. We explore incident threading as a possible solution to this problem. All text that describes the occurrence of a real-world happening is merged into a news incident, and incidents are organized in a network with dependencies of predefined types. Earlier attempts at this problem have assumed that a news story covers a single topic. We move beyond that limitation to introduce passage threading, which processes news at the passage level. First we develop a new testbed for this research and extend the evaluation methods to address new granularity issues. Then a three-stage algorithm is described that identifies on-subject passages, groups them into incidents, and establishes links between related incidents. Finally, we observe significant improvement over earlier work when we optimize the harmonic mean of the appropriate evaluation measures. The resulting performance exceeds the level that a calibration study shows is necessary to support a reading comprehension task.',\n",
              "  'art_au_id': '81100140749;81342487699',\n",
              "  'art_au_text': 'Ao Feng;James Allan',\n",
              "  'art_inst_id': '-1;1026103',\n",
              "  'art_inst_text': 'Amazon.com, Seattle, WA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '1871675',\n",
              "  'proc_id': '1871437',\n",
              "  'art_pub_year': 2010,\n",
              "  'art_pub_date': '10/26/2010',\n",
              "  'art_title': 'Fast query expansion using approximations of relevance models',\n",
              "  'art_abstract': 'Pseudo-relevance feedback (PRF) improves search quality by expanding the query using terms from high-ranking documents from an initial retrieval. Although PRF can often result in large gains in effectiveness, running two queries is time consuming, limiting its applicability. We describe a PRF method that uses corpus pre-processing to achieve query-time speeds that are near those of the original queries. Specifically, Relevance Modeling, a language modeling based PRF method, can be recast to benefit substantially from finding pairwise document relationships in advance. Using the resulting Fast Relevance Model (fastRM), we substantially reduce the online retrieval time and still benefit from expansion. We further explore methods for reducing the preprocessing time and storage requirements of the approach, allowing us to achieve up to a 10\\\\% increase in MAP over unexpanded retrieval,vwhile only requiring 1\\\\% of the time of standard expansion.',\n",
              "  'art_au_id': '81384621996;81342487699;81100151204;81100422581',\n",
              "  'art_au_text': 'Marc-Allen Cartright;James Allan;Victor Lavrenko;Andrew McGregor',\n",
              "  'art_inst_id': '1026103;1026103;-1;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;Unversity of Edinburgh, Edinburgh, United Kingdom;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '1931455',\n",
              "  'proc_id': '1931390',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '05/30/2007',\n",
              "  'art_title': 'Information retrieval techniques for templated queries',\n",
              "  'art_abstract': \"Queries in template form are gaining in popularity as a means of conveying specific information needs to search engines. We explore the utility of Information Retrieval (IR) techniques in the context of templated queries. Our investigations show that IR techniques known to be well-suited for ad hoc retrieval don't seamlessly extend to the case of templated queries. We show that what works is a combination of IR techniques and intuition-driven modifications to the templated queries, resulting in statistically significant improvements over the baseline.\",\n",
              "  'art_au_id': '81340490736;81342487699',\n",
              "  'art_au_text': 'Giridhar Kumaran;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1931456',\n",
              "  'proc_id': '1931390',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '05/30/2007',\n",
              "  'art_title': 'Discovering missing values in semi-structured databases',\n",
              "  'art_abstract': 'We explore the problem of discovering multiple missing values in a semi-structured database. For this task, we formally develop Structured Relevance Model (SRM) built on one hypothetical generative model for semi-structured records. SRM is based on the idea that plausible values for a given field could be inferred from the context provided by the other fields in the record. Small-scale experiments on IMDb (Internet Movie Database) show that SRM matched three state-of-the-art relational learning approaches on the movie label prediction tasks. Large-scale experiments on a snapshot of the National Science Digital Library (NSDL) repository show that SRM is highly effective at discovering possible values for free-text fields even with quite modest amounts of training data, compared with state-of-the-art machine learning approaches.',\n",
              "  'art_au_id': '81100168709;81342487699;81100151204',\n",
              "  'art_au_text': 'Xing Yi;James Allan;Victor Lavrenko',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'Drive University of Massachusetts, Amherst, MA;Drive University of Massachusetts, Amherst, MA;Drive University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '1931461',\n",
              "  'proc_id': '1931390',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '05/30/2007',\n",
              "  'art_title': 'Research methodology in studies of assessor effort for information retrieval evaluation',\n",
              "  'art_abstract': 'As evaluation is an important but difficult part of information retrieval system design and experimentation, evaluation questions have been the subject of much research. An \"evaluation study\" is an investigation into some aspect of evaluation. These types of studies typically experiment on ranked results from actual retrieval systems, most often those that were submitted to TREC tracks. We argue that the standard of evidence in these types of studies should be increased to the level required of text retrieval studies, by testing on multiple data sets, multiple subsets of data, and comparison to baselines using hypothesis testing. We demonstrate that baseline performance on the standard data sets is quite high, necessitating strong evidence to support claims.',\n",
              "  'art_au_id': '81100270582;81342487699',\n",
              "  'art_au_text': 'Ben Carterette;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '2063604',\n",
              "  'proc_id': '2063576',\n",
              "  'art_pub_year': 2011,\n",
              "  'art_pub_date': '10/24/2011',\n",
              "  'art_title': 'Discovering missing click-through query language information for web search',\n",
              "  'art_abstract': \"The click-through information in web query logs has been widely used for web search tasks. However, it usually suffers from the data sparseness problem, known as the missing/incomplete click problems, where large volume of pages receive few or no clicks. In this paper, we adapt two language modeling based approaches to address this issue in the context of using web query logs for web search. The first approach discovers missing click-through query language features for web pages with no or few clicks from their similar pages' click-associated queries in the query logs, to help search. We further propose combining this content based approach with the random walk approach on the click graph to further reduce click-through sparseness for search. The second approach follows the query expansion method and utilizes the queries and their clicked web pages in the query logs to reconstruct a structured variant of the relevance based language models for each user-input query for search. We design experiments with a publicly available query log excerpt and two TREC web search tasks on the GOV2 and ClueWeb09 corpora to evaluate the search performance of different approaches. Our results show that using discovered semantic click-through query language features can statistically significantly improve search performance, compared with the baselines that do not use the discovered information. The combination approach that uses discovered click-through features from both random walk and the content based approach can further improve search performance.\",\n",
              "  'art_au_id': '81100168709;81490653304',\n",
              "  'art_au_text': 'Xing Yi;James Allan',\n",
              "  'art_inst_id': '-1;-1',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, AMHERST, MA, USA;University of Massachusetts, Amherst, AMHERST, MA, USA'},\n",
              " {'article_id': '2063606',\n",
              "  'proc_id': '2063576',\n",
              "  'art_pub_year': 2011,\n",
              "  'art_pub_date': '10/24/2011',\n",
              "  'art_title': 'Reranking search results for sparse queries',\n",
              "  'art_abstract': \"It is well known that clickthrough data can be used to improve the effectiveness of search results: broadly speaking, a query's past clicks are a predictor of future clicks on documents. However, when a new or unusual query appears, or when a system is not as widely used as a mainstream web search system, there may be little to no click data available to improve the results. Existing methods to boost query performance for sparse queries extend the query-document click relationship to more documents or queries, but require substantial clickthrough data from other queries. In this work we describe a way to boost rarely-clicked queries in a system where limited clickthrough data is available for all queries. We describe a probabilistic approach for carrying out that estimation and use it to rerank retrieved documents. We utilize information from co-click queries, subset queries, and synonym queries to estimate the clickthrough for a sparse query. Our experiments on a query log from a medical informatics company demonstrate that when overall clickthrough data is sparse, reranking search results using clickthrough information from related queries significantly outperforms reranking that employs clickthrough information from the query alone.\",\n",
              "  'art_au_id': '81384621422;81342487699',\n",
              "  'art_au_text': 'Elif Aktolga;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2063625',\n",
              "  'proc_id': '2063576',\n",
              "  'art_pub_year': 2011,\n",
              "  'art_pub_date': '10/24/2011',\n",
              "  'art_title': 'Efficiency optimizations for interpolating subqueries',\n",
              "  'art_abstract': 'A large class of queries can be viewed as linear combinations of smaller subqueries. Additionally, many situations arise when part or all of one subquery has been preprocessed or has cached information, while another subquery requires full processing. This type of query is common, for example, in relevance feedback settings where the original query has been run to produce a set of expansion terms, but the expansion terms still need to be processed. We investigate mechanisms to reduce the time needed to process queries of this nature. We use RM3, a variant of the Relevance Model scoring algorithm, as our instantiation of this arrangement. We examine the different scenarios that can arise when we have access to the internal structure of each subquery. Given this additional information, we investigate methods to utilize this information, reducing processing costs substantially. Depending on the amount of accessibility we have into the subqueries, we can reduce processing costs over 80\\\\% without affecting the score of the final results.',\n",
              "  'art_au_id': '81490664238;81490653303',\n",
              "  'art_au_text': 'Marc-Allen Cartright;James Allan',\n",
              "  'art_inst_id': '1026103;-1',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massahusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2063633',\n",
              "  'proc_id': '2063576',\n",
              "  'art_pub_year': 2011,\n",
              "  'art_pub_date': '10/24/2011',\n",
              "  'art_title': 'Passage retrieval for incorporating global evidence in sequence labeling',\n",
              "  'art_abstract': 'Many forms of linguistic analysis, such as part of speech tagging, named entity recognition, and other sequence labeling tasks are performed on short spans of text and assume statistical dependence within a window of only a few tokens. We propose using passage retrieval to induce non-local dependencies in structured classification that generalizes earlier work in context aggregation for named-entity recognition. We introduce a new method for feature expansion inspired by psuedo-relevance feedback (PRF). Our results on the CoNLL 2003 task show that features from cross-document feature expansion improves NER effectiveness over previous aggregation models. Utilizing all the tokens in a sentence for query context consistently perform best on both intrinsic and extrinsic evaluations. Tagging models incorporating feature expansion outperform the leading NER system when evaluated on out of domain data, a collection of publicly available scanned books on the topic of historic Deerfield, MA. Finally, the results show that retrieval based feature expansion using an external collection of unlabeled text can result in further effectiveness improvements.',\n",
              "  'art_au_id': '81438596204;81490653302;81407592568',\n",
              "  'art_au_text': 'Jeffrey Dalton;James Allan;David A. Smith',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2064063',\n",
              "  'proc_id': '2064058',\n",
              "  'art_pub_year': 2011,\n",
              "  'art_pub_date': '10/24/2011',\n",
              "  'art_title': 'Evidence finding using a collection of books',\n",
              "  'art_abstract': 'This paper introduces the task of Evidence Finding , a novel information retrieval task that uses books - a traditionally more trust-worthy source of information - to help provide evidence to support a statement. What makes this evidence-finding task different from other tasks, such as the related INEX Prove It task, is that both the statement for which evidence is sought and its context are given to the search system. A practical application of this system is to provide supporting or refuting evidence from books for a statement made within a Wikipedia article, using the entire article as contextual support for query generation. We provide details of this task as well as an analysis of a number of retrieval methods that address this task.',\n",
              "  'art_au_id': '81384621996;81323490009;81490653301',\n",
              "  'art_au_text': 'Marc-Allen Cartright;Henry A. Feild;James Allan',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2064069',\n",
              "  'proc_id': '2064058',\n",
              "  'art_pub_year': 2011,\n",
              "  'art_pub_date': '10/24/2011',\n",
              "  'art_title': 'Mining relational structure from millions of books: position paper',\n",
              "  'art_abstract': 'Existing large-scale scanned book collections have many shortcomings for data-driven research, from OCR of variable quality to the lack of accurate descriptive and structural metadata. We argue that complementary research in inferring relational metadata is important in its own right to support use of these collections and that it can help to mitigate other problems with scanned book collections.',\n",
              "  'art_au_id': '81407592568;81460647657;81490653300',\n",
              "  'art_au_text': 'David A. Smith;R. Manmatha;James Allan',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, Amherst, MA, USA;University of Massachusetts, Amherst, Amherst, MA, USA;University of Massachusetts, Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '1099723',\n",
              "  'proc_id': '1099554',\n",
              "  'art_pub_year': 2005,\n",
              "  'art_pub_date': '10/31/2005',\n",
              "  'art_title': 'Incremental test collections',\n",
              "  'art_abstract': \"Corpora and topics are readily available for information retrieval research. Relevance judgments, which are necessary for system evaluation, are expensive; the cost of obtaining them prohibits in-house evaluation of retrieval systems on new corpora or new topics. We present an algorithm for cheaply constructing sets of relevance judgments. Our method intelligently selects documents to be judged and decides when to stop in such a way that with very little work there can be a high degree of confidence in the result of the evaluation. We demonstrate the algorithm's effectiveness by showing that it produces small sets of relevance judgments that reliably discriminate between two systems. The algorithm can be used to incrementally design retrieval systems by simultaneously comparing sets of systems. The number of additional judgments needed after each incremental design change decreases at a rate reciprocal to the number of systems being compared. To demonstrate the effectiveness of our method, we evaluate TREC ad hoc submissions, showing that with 95\\\\% fewer relevance judgments we can reach a Kendall's tau rank correlation of at least 0.9.\",\n",
              "  'art_au_id': '81100270582;81342487699',\n",
              "  'art_au_text': 'Ben Carterette;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1031258',\n",
              "  'proc_id': '1031171',\n",
              "  'art_pub_year': 2004,\n",
              "  'art_pub_date': '11/13/2004',\n",
              "  'art_title': 'Event threading within news topics',\n",
              "  'art_abstract': 'With the overwhelming volume of online news available today, there is an increasing need for automatic techniques to analyze and present news to the user in a meaningful and efficient manner. Previous research focused only on organizing news stories by their topics into a flat hierarchy. We believe viewing a news topic as a flat collection of stories is too restrictive and inefficient for a user to understand the topic quickly. In this work, we attempt to capture the rich structure of events and their dependencies in a news topic through our event models. We call the process of recognizing events and their dependencies <i>event threading</i>. We believe our perspective of modeling the structure of a topic is more effective in capturing its semantics than a flat list of on-topic stories. We formally define the novel problem, suggest evaluation metrics and present a few techniques for solving the problem. Besides the standard word based features, our approaches take into account novel features such as temporal locality of stories for event recognition and time-ordering for capturing dependencies. Our experiments on a manually labeled data sets show that our models effectively identify the events and capture dependencies among them.',\n",
              "  'art_au_id': '81100451338;81100140749;81100050257;81342487699',\n",
              "  'art_au_text': 'Ramesh Nallapati;Ao Feng;Fuchun Peng;James Allan',\n",
              "  'art_inst_id': '1026103;1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '1321528',\n",
              "  'proc_id': '1321440',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '11/06/2007',\n",
              "  'art_title': 'A comparison of statistical significance tests for information retrieval evaluation',\n",
              "  'art_abstract': \"Information retrieval (IR) researchers commonly use three tests of statistical significance: the Student's paired t-test, the Wilcoxon signed rank test, and the sign test. Other researchers have previously proposed using both the bootstrap and Fisher's randomization (permutation) test as non-parametric significance tests for IR but these tests have seen little use. For each of these five tests, we took the ad-hoc retrieval runs submitted to TRECs 3 and 5-8, and for each pair of runs, we measured the statistical significance of the difference in their mean average precision. We discovered that there is little practical difference between the randomization, bootstrap, and t tests. Both the Wilcoxon and sign test have a poor ability to detect significance and have the potential to lead to false detections of significance. The Wilcoxon and sign tests are simplified variants of the randomization test and their use should be discontinued for measuring the significance of a difference between means.\",\n",
              "  'art_au_id': '81100457054;81342487699;81100270582',\n",
              "  'art_au_text': 'Mark D. Smucker;James Allan;Ben Carterette',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1321554',\n",
              "  'proc_id': '1321440',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '11/06/2007',\n",
              "  'art_title': 'Finding and linking incidents in news',\n",
              "  'art_abstract': 'News reports are being produced and disseminated in overwhelming volume, making it difficult to keep up with the newest information. Most previous research in automatic news organization treated news topics as a flat list, ignoring the intrinsic connection among individual reports. We argue that more contextual information within and across the topics will benefit users in their news understanding process. A news organization infrastructure, incident threading , is proposed in this article. All text snippets describing the occurrence of a real-world happening are combined into a news incident, and a network is composed of incidents that are interconnected by links in certain types. A limited vocabulary of connection types is defined and corresponding rules are established based upon the human experience of news understanding. The incident threading system is implemented with two different algorithms. One starts from clustering of text passages and then creates links with pre-built rules. The other method defines a global score function over the whole collection and solves the optimization problem with simulated annealing. The former achieves higher accuracy in the identification of incidents and the latter generates better links, which is preferred since the links are more important for the formation of the incident network.',\n",
              "  'art_au_id': '81100140749;81342487699',\n",
              "  'art_au_text': 'Ao Feng;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts,, Amherst, MA;University of Massachusetts,, Amherst, MA'},\n",
              " {'article_id': '1321564',\n",
              "  'proc_id': '1321440',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '11/06/2007',\n",
              "  'art_title': 'Semiautomatic evaluation of retrieval systems using document similarities',\n",
              "  'art_abstract': 'Semiautomatic evaluation of retrieval systems using document similarities.',\n",
              "  'art_au_id': '81100270582;81342487699',\n",
              "  'art_au_text': 'Ben Carterette;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1321576',\n",
              "  'proc_id': '1321440',\n",
              "  'art_pub_year': 2007,\n",
              "  'art_pub_date': '11/06/2007',\n",
              "  'art_title': 'Selective user interaction',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81340490736;81342487699',\n",
              "  'art_au_text': 'Giridhar Kumaran;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA;University of Massachusetts Amherst, Amherst, MA'},\n",
              " {'article_id': '1458179',\n",
              "  'proc_id': '1458082',\n",
              "  'art_pub_year': 2008,\n",
              "  'art_pub_date': '10/26/2008',\n",
              "  'art_title': 'Simultaneous multilingual search for translingual information retrieval',\n",
              "  'art_abstract': 'We consider the problem of translingual information retrieval, where monolingual searchers issue queries in a different language than the document language(s) and the results must be returned in the language they know, the query language. We present a framework for translingual IR that integrates document translation and query translation into the retrieval model. The corpus is represented as an aligned, jointly indexed \"pseudo-parallel\" corpus, where each document contains the text of the document along with its translation into the query language. The queries are formulated as multilingual structured queries, where each query term and its translations into the document language(s) are treated as synonym sets. This model leverages simultaneous search in multiple languages against jointly indexed documents to improve the accuracy of results over search using document translation or query translation alone. For query translation, we compared a statistical machine translation (SMT) approach to a dictionary-based approach. We found that using a Wikipedia-derived dictionary for named entities combined with an SMT-based dictionary worked better than SMT alone. Simultaneous multilingual search also has other important features suited to translingual search, since it can provide an indication of poor document translation when a match with the source document is found. We show how close integration of CLIR and SMT allows us to improve result translation in addition to IR results.',\n",
              "  'art_au_id': '81384603548;81100057436;81342487699;81384620098',\n",
              "  'art_au_text': 'Kristen Parton;Kathleen R. McKeown;James Allan;Enrique Henestroza',\n",
              "  'art_inst_id': '1005362;1005362;1026103;1005362',\n",
              "  'art_inst_text': 'Columbia University, New York, NY, USA;Columbia University, New York, NY, USA;University of Massachusetts Amherst, Amherst, MA, USA;Columbia University, New York, NY, USA'},\n",
              " {'article_id': '1458199',\n",
              "  'proc_id': '1458082',\n",
              "  'art_pub_year': 2008,\n",
              "  'art_pub_date': '10/26/2008',\n",
              "  'art_title': 'Joke retrieval: recognizing the same joke told differently',\n",
              "  'art_abstract': 'In a corpus of jokes, a human might judge two documents to be the \"same joke\" even if characters, locations, and other details are varied. A given joke could be retold with an entirely different vocabulary while still maintaining its identity. Since most retrieval systems consider documents to be related only when their word content is similar, we propose joke retrieval as a domain where standard language models may fail. Other meaning-centric domains include logic puzzles, proverbs and recipes; in such domains, new techniques may be required to enable us to search effectively. For jokes, a necessary component of any retrieval system will be the ability to identify the \"same joke,\" so we examine this task in both ranking and classification settings. We exploit the structure of jokes to develop two domain-specific alternatives to the \"bag of words\" document model. In one, only the punch lines, or final sentences, are compared; in the second, certain categories of words (e.g., professions and countries) are tagged and treated as interchangeable. Each technique works well for certain jokes. By combining the methods using machine learning, we create a hybrid that achieves higher performance than any individual approach.',\n",
              "  'art_au_id': '81335490542;81342487699',\n",
              "  'art_au_text': 'Lisa Friedland;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '1458278',\n",
              "  'proc_id': '1458082',\n",
              "  'art_pub_year': 2008,\n",
              "  'art_pub_date': '10/26/2008',\n",
              "  'art_title': 'Cross-document cross-lingual coreference retrieval',\n",
              "  'art_abstract': 'In this work, we address coreference retrieval, which involves identifying aliases that are distinct references to an entity. We begin with a known alias and discover unknown aliases that refer to the same entity. We use Entity Language Models to capture the contextual language around the known alias, which aids in finding new aliases. We also show that modeling the significant dates of the known aliases improves alias discovery performance.',\n",
              "  'art_au_id': '81384621422;81384621996;81342487699',\n",
              "  'art_au_text': 'Elif Aktolga;Marc-Allen Cartright;James Allan',\n",
              "  'art_inst_id': '1026103;1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '1458317',\n",
              "  'proc_id': '1458082',\n",
              "  'art_pub_year': 2008,\n",
              "  'art_pub_date': '10/26/2008',\n",
              "  'art_title': 'Evaluating topic models for information retrieval',\n",
              "  'art_abstract': \"We explore the utility of different types of topic models, both probabilistic and not, for retrieval purposes. We show that: (1) topic models are effective for document smoothing; (2) more elaborate topic models that capture topic dependencies provide no additional gains; (3) smoothing documents by using their similar documents is as effective as smoothing them by using topic models; (4) topics discovered on the whole corpus are too coarse-grained to be useful for query expansion. Experiments to measure topic models' ability to predict held-out likelihood confirm past results on small corpora, but suggest that simple approaches to topic model are better for large corpora.\",\n",
              "  'art_au_id': '81100168709;81342487699',\n",
              "  'art_au_text': 'Xing Yi;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, Amherst, MA, USA;University of Massachusetts, Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '234833',\n",
              "  'proc_id': '234828',\n",
              "  'art_pub_year': 1996,\n",
              "  'art_pub_date': '03/01/1996',\n",
              "  'art_title': 'Automatic hypertext link typing',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81342487699',\n",
              "  'art_au_text': 'James Allan',\n",
              "  'art_inst_id': '1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '319956',\n",
              "  'proc_id': '319950',\n",
              "  'art_pub_year': 1999,\n",
              "  'art_pub_date': '11/01/1999',\n",
              "  'art_title': 'Extracting significant time varying features from text',\n",
              "  'art_abstract': 'We propose a simple statistical model for the frequency of occurrence of features in a stream of text. Adoption of this model allows us to use classical significance tests to filter the stream for interesting events. We tested the model by building a system and running it on a news corpus. By a subjective evaluation, the system worked remarkably well: almost all of the groups of identified tokens corresponded to news stories and were appropriately placed in time. A preliminary objective evaluation was also used to measure the quality of the system and it showed some of the weaknesses and the power of our approach.',\n",
              "  'art_au_id': '81375604095;81342487699',\n",
              "  'art_au_text': 'Russell Swan;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, Massachusetts;Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, Massachusetts'},\n",
              " {'article_id': '288648',\n",
              "  'proc_id': '288627',\n",
              "  'art_pub_year': 1998,\n",
              "  'art_pub_date': '11/01/1998',\n",
              "  'art_title': 'Document classification using multiword features',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81100034951;81342487699',\n",
              "  'art_au_text': 'Ron Papka;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, MA;Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '354843',\n",
              "  'proc_id': '354756',\n",
              "  'art_pub_year': 2000,\n",
              "  'art_pub_date': '11/06/2000',\n",
              "  'art_title': 'First story detection in TDT is hard',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81342487699;81100151204;81375602759',\n",
              "  'art_au_text': 'James Allan;Victor Lavrenko;Hubert Jin',\n",
              "  'art_inst_id': '1026103;1026103;-1',\n",
              "  'art_inst_text': 'Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, Massachusetts;Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts, Amherst, Massachusetts;MapQuest.Com, Inc., Mountvile, Pennsylvania'},\n",
              " {'article_id': '354845',\n",
              "  'proc_id': '354756',\n",
              "  'art_pub_year': 2000,\n",
              "  'art_pub_date': '11/06/2000',\n",
              "  'art_title': 'Language models for financial news recommendation',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81100151204;81100097293;81100492102;81100143338;81100640360;81342487699',\n",
              "  'art_au_text': 'Victor Lavrenko;Matt Schmill;Dawn Lawrie;Paul Ogilvie;David Jensen;James Allan',\n",
              "  'art_inst_id': '1026103;1026103;1026103;1026103;1026103;1026103',\n",
              "  'art_inst_text': 'Department of Computer Science, University of Massachusetts, Amherst, MA;Department of Computer Science, University of Massachusetts, Amherst, MA;Department of Computer Science, University of Massachusetts, Amherst, MA;Department of Computer Science, University of Massachusetts, Amherst, MA;Department of Computer Science, University of Massachusetts, Amherst, MA;Department of Computer Science, University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '584855',\n",
              "  'proc_id': '584792',\n",
              "  'art_pub_year': 2002,\n",
              "  'art_pub_date': '11/04/2002',\n",
              "  'art_title': 'Capturing term dependencies using a language model based on sentence trees',\n",
              "  'art_abstract': \"We describe a new probabilistic Sentence Tree Language Modeling approach that captures term dependency patterns in Topic Detection and Tracking's (TDT) Story Link Detection task. New features of the approach include modeling the syntactic structure of sentences in documents by a sentence-bin approach and a computationally efficient algorithm for capturing the most significant sentence-level term dependencies using a Maximum Spanning Tree approach, similar to Van Rijsbergen's modeling of document-level term dependencies.The new model is a good discriminator of on-topic and off-topic story pairs providing evidence that sentence-level term dependencies contain significant information about relevance. Although runs on a subset of the TDT2 corpus show that the model is outperformed by the unigram language model, a mixture of the unigram and the Sentence Tree models is shown to improve on the best performance especially in the regions of low false alarms.\",\n",
              "  'art_au_id': '81100451338;81342487699',\n",
              "  'art_au_text': 'Ramesh Nallapati;James Allan',\n",
              "  'art_inst_id': '1026103;1026103',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, MA;University of Massachusetts, Amherst, MA'},\n",
              " {'article_id': '956914',\n",
              "  'proc_id': '956863',\n",
              "  'art_pub_year': 2003,\n",
              "  'art_pub_date': '11/03/2003',\n",
              "  'art_title': 'Flexible intrinsic evaluation of hierarchical clustering for TDT',\n",
              "  'art_abstract': 'The Topic Detection and Tracking (TDT) evaluation program has included a \"cluster detection\" task since its inception in 1996. Systems were required to process a stream of broadcast news stories and partition them into non-overlapping clusters. A system\\'s effectiveness was measured by comparing the generated clusters to \"truth\" clusters created by human annotators. Starting in 2003, TDT is moving to a more realistic model that permits overlapping clusters (stories may be on more than one topic) and encourages the creation of a hierarchy to structure the relationships between clusters (topics). We explore a range of possible evaluation models for this modified TDT clustering task to understand the best approach for mapping between the human-generated \"truth\" clusters and a much richer hierarchical structure. We demonstrate that some obvious evaluation techniques fail for degenerate cases. For a few others we attempt to develop an intuitive sense of what the evaluation numbers mean. We settle on some approaches that incorporate a strong balance between cluster errors (misses and false alarms) and the distance it takes to travel between stories within the hierarchy.',\n",
              "  'art_au_id': '81342487699;81100140749;81100124635',\n",
              "  'art_au_text': 'James Allan;Ao Feng;Alvaro Bolivar',\n",
              "  'art_inst_id': '-1;-1;-1',\n",
              "  'art_inst_text': 'University of Massachusetts, MA;University of Massachusetts, MA;University of Massachusetts, MA'},\n",
              " {'article_id': '956973',\n",
              "  'proc_id': '956863',\n",
              "  'art_pub_year': 2003,\n",
              "  'art_pub_date': '11/03/2003',\n",
              "  'art_title': 'Relevant query feedback in statistical language modeling',\n",
              "  'art_abstract': \"In traditional relevance feedback, researchers have explored relevant document feedback, wherein, the query representation is updated based on a set of relevant documents returned by the user. In this work, we investigate relevant query feedback, in which we update a document's representation based on a set of relevant queries. We propose four statistical models to incorporate relevant query feedback.To validate our models, we considered anchor text of incoming links to a given document as feedback queries and performed experiments on the home-page retrieval task of TREC 2001. Our results show that three of our four models outperform the query-likelihood baseline by at least 35\\\\% in MRR score on a test set.\",\n",
              "  'art_au_id': '81100451338;81100651098;81342487699',\n",
              "  'art_au_text': 'Ramesh Nallapati;Bruce Croft;James Allan',\n",
              "  'art_inst_id': '-1;-1;-1',\n",
              "  'art_inst_text': 'University of Massachusetts;University of Massachusetts;University of Massachusetts'},\n",
              " {'article_id': '2390122',\n",
              "  'proc_id': '2390116',\n",
              "  'art_pub_year': 2012,\n",
              "  'art_pub_date': '10/29/2012',\n",
              "  'art_title': 'Search and exploration of scanned books',\n",
              "  'art_abstract': 'In this demo, we present Proteus, a novel interface for interacting with multiple retrieval types extracted from scanned books provided by the Internet Archive. The primary purpose of Proteus is to provide a rich interactive experience for users to explore collections with automatically extracted and linked entity data. The system supports seamlessly shifting perspectives between books, entities, and topics. Proteus provides a starting point for a variety of exploratory search tasks.',\n",
              "  'art_au_id': '81384621996;81438596204;81342487699',\n",
              "  'art_au_text': 'Marc-Allen Cartright;Jeffrey Dalton;James Allan',\n",
              "  'art_inst_id': '60014313;60014313;60014313',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2348426',\n",
              "  'proc_id': '2348283',\n",
              "  'art_pub_year': 2012,\n",
              "  'art_pub_date': '08/12/2012',\n",
              "  'art_title': 'A framework for manipulating and searching multiple retrieval types',\n",
              "  'art_abstract': 'Conventional retrieval systems view documents as a unit and look at different retrieval types within a document. We introduce Proteus, a frame-work for seamlessly navigating books as dynamic collections which are defined on the fly. Proteus allows us to search various retrieval types. Navigable types include pages, books, named persons, locations, and pictures in a collection of books taken from the Internet Archive. The demonstration shows the value of multi-type browsing in dynamic collections to peruse new data.',\n",
              "  'art_au_id': '81384621996;81490669130;81443598258;81438596204;81508708550;81496651588;81508703883;81453657337;81342487699;81460647657;81407592568',\n",
              "  'art_au_text': 'Marc-Allen Cartright;Ethem F. Can;William Dabney;Jeff Dalton;Logan Giorda;Kriste Krstovski;Xiaoye Wu;Ismet Zeki Yalniz;James Allan;R. Manmatha;David A. Smith',\n",
              "  'art_inst_id': '60014313;60014313;60014313;60014313;60014313;60014313;60014313;60014313;60014313;60014313;60014313',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2348440',\n",
              "  'proc_id': '2348283',\n",
              "  'art_pub_year': 2012,\n",
              "  'art_pub_date': '08/12/2012',\n",
              "  'art_title': 'Task-aware search assistant',\n",
              "  'art_abstract': 'An abstract is not available.',\n",
              "  'art_au_id': '81323490009;81548042759',\n",
              "  'art_au_text': 'Henry Allen Feild;James Allan',\n",
              "  'art_inst_id': '60014313;60014313',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2362735',\n",
              "  'proc_id': '2362724',\n",
              "  'art_pub_year': 2012,\n",
              "  'art_pub_date': '08/21/2012',\n",
              "  'art_title': 'Human question answering performance using an interactive document retrieval system',\n",
              "  'art_abstract': \"Every day, people answer their questions by using document retrieval systems. Compared to document retrieval systems, question answering (QA) systems aim to speed the rate at which users find answers by retrieving answers rather than documents. To better understand how document retrieval systems compare to QA systems, we measured the performance of humans using an interactive document retrieval system to answer questions. We first measured the ability of users to answer their questions using an interactive document retrieval system, and then compared the users' performance with the document retrieval system to question answering systems. We found that while users can successfully answer their questions using a document retrieval system, question answering systems have the potential to significantly increase the rate at which users find answers.\",\n",
              "  'art_au_id': '81100457054;81548042759;81548042760',\n",
              "  'art_au_text': 'Mark D. Smucker;James Allan;Blagovest Dachev',\n",
              "  'art_inst_id': '60014171;60014313;-1',\n",
              "  'art_inst_text': 'University of Waterloo;University of Massachusetts Amherst;TST Media, Minneapolis, MN'},\n",
              " {'article_id': '2484097',\n",
              "  'proc_id': '2484028',\n",
              "  'art_pub_year': 2013,\n",
              "  'art_pub_date': '07/28/2013',\n",
              "  'art_title': 'Extracting query facets from search results',\n",
              "  'art_abstract': \"Web search queries are often ambiguous or multi-faceted, which makes a simple ranked list of results inadequate. To assist information finding for such faceted queries, we explore a technique that explicitly represents interesting facets of a query using groups of semantically related terms extracted from search results. As an example, for the query ``baggage allowance'', these groups might be different airlines, different flight types (domestic, international), or different travel classes (first, business, economy). We name these groups query facets and the terms in these groups facet terms. We develop a supervised approach based on a graphical model to recognize query facets from the noisy candidates found. The graphical model learns how likely a candidate term is to be a facet term as well as how likely two terms are to be grouped together in a query facet, and captures the dependencies between the two factors. We propose two algorithms for approximate inference on the graphical model since exact inference is intractable. Our evaluation combines recall and precision of the facet terms with the grouping quality. Experimental results on a sample of web queries show that the supervised method significantly outperforms existing approaches, which are mostly unsupervised, suggesting that query facet extraction can be effectively learned.\",\n",
              "  'art_au_id': '-1;-1',\n",
              "  'art_au_text': 'Weize Kong;James Allan',\n",
              "  'art_inst_id': '-1;-1',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2484069',\n",
              "  'proc_id': '2484028',\n",
              "  'art_pub_year': 2013,\n",
              "  'art_pub_date': '07/28/2013',\n",
              "  'art_title': 'Task-aware query recommendation',\n",
              "  'art_abstract': \"When generating query recommendations for a user, a natural approach is to try and leverage not only the user's most recently submitted query, or reference query, but also information about the current search context, such as the user's recent search interactions. We focus on two important classes of queries that make up search contexts: those that address the same information need as the reference query (on-task queries), and those that do not (off-task queries). We analyze the effects on query recommendation performance of using contexts consisting of only on-task queries, only off-task queries, and a mix of the two. Using TREC Session Track data for simulations, we demonstrate that on-task context is helpful on average but can be easily overwhelmed when off-task queries are interleaved---a common situation according to several analyses of commercial search logs. To minimize the impact of off-task queries on recommendation performance, we consider automatic methods of identifying such queries using a state of the art search task identification technique. Our experimental results show that automatic search task identification can eliminate the effect of off-task queries in a mixed context. We also introduce a novel generalized model for generating recommendations over a search context. While we only consider query text in this study, the model can handle integration over arbitrary user search behavior, such as page visits, dwell times, and query abandonment. In addition, it can be used for other types of recommendation, including personalized web search.\",\n",
              "  'art_au_id': '-1;-1',\n",
              "  'art_au_text': 'Henry Feild;James Allan',\n",
              "  'art_inst_id': '-1;-1',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, Massachusetts, USA;University of Massachusetts, Amherst, Massachusetts, USA'},\n",
              " {'article_id': '2484060',\n",
              "  'proc_id': '2484028',\n",
              "  'art_pub_year': 2013,\n",
              "  'art_pub_date': '07/28/2013',\n",
              "  'art_title': 'Sentiment diversification with different biases',\n",
              "  'art_abstract': \"Prior search result diversification work focuses on achieving topical variety in a ranked list, typically equally across all aspects. In this paper, we diversify with sentiments according to an explicit bias. We want to allow users to switch the result perspective to better grasp the polarity of opinionated content, such as during a literature review. For this, we first infer the prior sentiment bias inherent in a controversial topic -- the 'Topic Sentiment'. Then, we utilize this information in 3 different ways to diversify results according to various sentiment biases: (1) Equal diversification to achieve a balanced and unbiased representation of all sentiments on the topic; (2) Diversification towards the Topic Sentiment, in which the actual sentiment bias in the topic is mirrored to emphasize the general perception of the topic; (3) Diversification against the Topic Sentiment, in which documents about the 'minority' or outlying sentiment(s) are boosted and those with the popular sentiment are demoted. Since sentiment classification is an essential tool for this task, we experiment by gradually degrading the accuracy of a perfect classifier down to 40%, and show which diversification approaches prove most stable in this setting. The results reveal that the proportionality-based methods and our SCSF model, considering sentiment strength and frequency in the diversified list, yield the highest gains. Further, in case the Topic Sentiment cannot be reliably estimated, we show how performance is affected by equal diversification when actually an emphasis either towards or against the Topic Sentiment is desired: in the former case, an average of 6.48% is lost across all evaluation measures, whereas in the latter case this is 16.23%, confirming that bias-specific sentiment diversification is crucial.\",\n",
              "  'art_au_id': '-1;-1',\n",
              "  'art_au_text': 'Elif Aktolga;James Allan',\n",
              "  'art_inst_id': '-1;-1',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2507880',\n",
              "  'proc_id': '2505515',\n",
              "  'art_pub_year': 2013,\n",
              "  'art_pub_date': '10/27/2013',\n",
              "  'art_title': 'Zero-shot video retrieval using content and concepts',\n",
              "  'art_abstract': \"Recent research in video retrieval has been successful at finding videos when the query consists of tens or hundreds of sample relevant videos for training supervised models. Instead, we investigate unsupervised zero-shot retrieval where no training videos are provided: a query consists only of a text statement. For retrieval, we use text extracted from images in the videos, text recognized in the speech of its audio track, as well as automatically detected semantically meaningful visual video concepts identified with widely varying confidence in the videos. In this work we introduce a new method for automatically identifying relevant concepts given a text query using the Markov Random Field (MRF) retrieval framework. We use source expansion to build rich textual representations of semantic video concepts from large external sources such as the web. We find that concept-based retrieval significantly outperforms text based approaches in recall. Using an evaluation derived from the TRECVID MED'11 track, we present early results that an approach using multi-modal fusion can compensate for inadequacies in each modality, resulting in substantial effectiveness gains. With relevance feedback, our approach provides additional improvements of over 50%.\",\n",
              "  'art_au_id': '81438596204;81342487699;81392599930',\n",
              "  'art_au_text': 'Jeffrey Dalton;James Allan;Pranav Mirajkar',\n",
              "  'art_inst_id': '60014313;60014313;60014313',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2507877',\n",
              "  'proc_id': '2505515',\n",
              "  'art_pub_year': 2013,\n",
              "  'art_pub_date': '10/27/2013',\n",
              "  'art_title': 'Detecting controversy on the web',\n",
              "  'art_abstract': \"A useful feature to facilitate critical literacy would alert users when they are reading a controversial web page. This requires solving a binary classification problem: does a given web page discuss a controversial topic? We explore the feasibility of solving the problem by treating it as supervised k-nearest-neighbor classification. Our approach (1) maps a webpage to a set of neighboring Wikipedia articles which were labeled on a controversiality metric; (2) coalesces those labels into an estimate of the webpage's controversiality; and finally (3) converts the estimate to a binary value using a threshold. We demonstrate the applicability of our approach by validating it on a set of webpages drawn from seed queries. We show absolute gains of 22% in F_0.5 on our test set over a sentiment-based approach, highlighting that detecting controversy is more complex than simply detecting opinions.\",\n",
              "  'art_au_id': '81467670473;81342487699',\n",
              "  'art_au_text': 'Shiri Dori-Hacohen;James Allan',\n",
              "  'art_inst_id': '60014313;60014313',\n",
              "  'art_inst_text': 'University of Massachusetts, Amherst, Amherst, MA, USA;University of Massachusetts, Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2505719',\n",
              "  'proc_id': '2505515',\n",
              "  'art_pub_year': 2013,\n",
              "  'art_pub_date': '10/27/2013',\n",
              "  'art_title': 'Improving passage ranking with user behavior information',\n",
              "  'art_abstract': 'User behavior information has proved valuable for inferring document relevance, but its role in deducing relevance at the passage/section level is not well explored. In this paper, we study how user behavior information implies section relevance, and use this information to improve section ranking. More specifically, we focus on four types of user search behavior that occur while browsing a document -- dwell time, highlighting, copying and clicks at the section level. Experimental results based on a commercial query log show that user behavior information can significantly improve section ranking. While section-level click information is a very powerful signal of relevance, it depends on an interface supporting section-level links. We find comparable levels of gain using other behavior information that does not depend upon such an interface.',\n",
              "  'art_au_id': '81414611776;81384621422;81342487699',\n",
              "  'art_au_text': 'Weize Kong;Elif Aktolga;James Allan',\n",
              "  'art_inst_id': '60014313;60014313;60014313',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2513164',\n",
              "  'proc_id': '2513150',\n",
              "  'art_pub_year': 2013,\n",
              "  'art_pub_date': '11/01/2013',\n",
              "  'art_title': 'Using CrowdLogger for in situ information retrieval system evaluation',\n",
              "  'art_abstract': 'A major hurdle faced by many information retrieval researchers---especially in academia---is evaluating retrieval systems in the wild. Challenges include tapping into large user bases, collecting user behavior, and modifying a given retrieval system. We outline several options available to researchers to overcome these challenges along with their advantages and disadvantages. We then demonstrate how CrowdLogger, an open-source browser extension for Firefox and Google Chrome, can be used as an in situ evaluation platform.',\n",
              "  'art_au_id': '81323490009;81342487699',\n",
              "  'art_au_text': 'Henry A. Feild;James Allan',\n",
              "  'art_inst_id': '60099445;60014313',\n",
              "  'art_inst_text': 'Endicott College, Beverly, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2609628',\n",
              "  'proc_id': '2600428',\n",
              "  'art_pub_year': 2014,\n",
              "  'art_pub_date': '07/03/2014',\n",
              "  'art_title': 'Entity query feature expansion using knowledge base links',\n",
              "  'art_abstract': 'Recent advances in automatic entity linking and knowledge base construction have resulted in entity annotations for document and query collections. For example, annotations of entities from large general purpose knowledge bases, such as Freebase and the Google Knowledge Graph. Understanding how to leverage these entity annotations of text to improve ad hoc document retrieval is an open research area. Query expansion is a commonly used technique to improve retrieval effectiveness. Most previous query expansion approaches focus on text, mainly using unigram concepts. In this paper, we propose a new technique, called entity query feature expansion (EQFE) which enriches the query with features from entities and their links to knowledge bases, including structured attributes and text. We experiment using both explicit query entity annotations and latent entities. We evaluate our technique on TREC text collections automatically annotated with knowledge base entity links, including the Google Freebase Annotations (FACC1) data. We find that entity-based feature expansion results in significant improvements in retrieval effectiveness over state-of-the-art text expansion approaches.',\n",
              "  'art_au_id': '-1;-1;-1',\n",
              "  'art_au_text': 'Jeffrey Dalton;Laura Dietz;James Allan',\n",
              "  'art_inst_id': '-1;-1;-1',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2609633',\n",
              "  'proc_id': '2600428',\n",
              "  'art_pub_year': 2014,\n",
              "  'art_pub_date': '07/03/2014',\n",
              "  'art_title': 'Searching, browsing, and clicking in a search session: changes in user behavior by task and over time',\n",
              "  'art_abstract': 'There are many existing studies of user behavior in simple tasks (e.g., navigational and informational search) within a short duration of 1--2 queries. However, we know relatively little about user behavior, especially browsing and clicking behavior, for longer search session solving complex search tasks. In this paper, we characterize and compare user behavior in relatively long search sessions (10 minutes; about 5 queries) for search tasks of four different types. The tasks differ in two dimensions: (1) the user is locating facts or is pursuing intellectual understanding of a topic; (2) the user has a specific task goal or has an ill-defined and undeveloped goal. We analyze how search behavior as well as browsing and clicking patterns change during a search session in these different tasks. Our results indicate that user behavior in the four types of tasks differ in various aspects, including search activeness, browsing style, clicking strategy, and query reformulation. As a search session progresses, we note that users shift their interests to focus less on the top results but more on results ranked at lower positions in browsing. We also found that results eventually become less and less attractive for the users. The reasons vary and include downgraded search performance of query, decreased novelty of search results, and decaying persistence of users in browsing. Our study highlights the lack of long session support in existing search engines and suggests different strategies of supporting longer sessions according to different task types.',\n",
              "  'art_au_id': '-1;-1;-1',\n",
              "  'art_au_text': 'Jiepu Jiang;Daqing He;James Allan',\n",
              "  'art_inst_id': '-1;-1;-1',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Pittsburgh, Pittsburgh, PA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2609536',\n",
              "  'proc_id': '2600428',\n",
              "  'art_pub_year': 2014,\n",
              "  'art_pub_date': '07/03/2014',\n",
              "  'art_title': 'Necessary and frequent terms in queries',\n",
              "  'art_abstract': 'Vocabulary mismatch has long been recognized as one of the major issues affecting search effectiveness. Ineffective queries usually fail to incorporate important terms and/or incorrectly include inappropriate keywords. However, in this paper we show another cause of reduced search performance: sometimes users issue reasonable query terms, but systems cannot identify the correct properties of those terms and take advantages of the properties. Specifically, we study two distinct types of terms that exist in all search queries: (1) necessary terms, for which term occurrence alone is indicative of document relevance; and (2) frequent terms, for which the relative term frequency is indicative of document relevance within the set of documents where the term appears. We evaluate these two properties of query terms in a dataset. Results show that only 1/3 of the terms are both necessary and frequent, while another 1/3 only hold one of the properties and the final third do not hold any of the properties. However, existing retrieval models do not clearly distinguish terms with the two properties and consider them differently. We further show the great potential of improving retrieval models by treating terms with distinct properties differently.',\n",
              "  'art_au_id': '-1;-1',\n",
              "  'art_au_text': 'Jiepu Jiang;James Allan',\n",
              "  'art_inst_id': '-1;-1',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'},\n",
              " {'article_id': '2661964',\n",
              "  'proc_id': '2661829',\n",
              "  'art_pub_year': 2014,\n",
              "  'art_pub_date': '11/03/2014',\n",
              "  'art_title': 'Extending Faceted Search to the General Web',\n",
              "  'art_abstract': 'Faceted search helps users by offering drill-down options as a complement to the keyword input box, and it has been used successfully for many vertical applications, including e-commerce and digital libraries. However, this idea is not well explored for general web search, even though it holds great potential for assisting multi-faceted queries and exploratory search. In this paper, we explore this potential by extending faceted search into the open-domain web setting, which we call Faceted Web Search. To tackle the heterogeneous nature of the web, we propose to use query-dependent automatic facet generation, which generates facets for a query instead of the entire corpus. To incorporate user feedback on these query facets into document ranking, we investigate both Boolean filtering and soft ranking models. We evaluate Faceted Web Search systems by their utility in assisting users to clarify search intent and find subtopic information. We describe how to build reusable test collections for such tasks, and propose an evaluation method that considers both gain and cost for users. Our experiments testify to the potential of Faceted Web Search, and show Boolean filtering feedback models, which are widely used in conventional faceted search, are less effective than soft ranking models.',\n",
              "  'art_au_id': '81414611776;81342487699',\n",
              "  'art_au_text': 'Weize Kong;James Allan',\n",
              "  'art_inst_id': '60014313;60014313',\n",
              "  'art_inst_text': 'University of Massachusetts Amherst, Amherst, MA, USA;University of Massachusetts Amherst, Amherst, MA, USA'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "5UydpeoeL2nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672f9268-abe8-4555-c9b2-5846c6132b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OEW6K6JzL2kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZIcM9hqL2hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_khRfAlL2ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pSCQcuzEL2bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nf6L8aq_L2Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oq6kdNJqL2WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXy4FB61L2Th"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}